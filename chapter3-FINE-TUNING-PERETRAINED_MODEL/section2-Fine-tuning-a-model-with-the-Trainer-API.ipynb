{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mojtaba-Choopani/huggingface-llm-course-fa-notebooks/blob/main/chapter3-FINE-TUNING-PERETRAINED_MODEL/section2-Fine-tuning-a-model-with-the-Trainer-API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TZbKROGjGJY"
      },
      "source": [
        "# Fine-tuning a model with the Trainer API\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    کتابخانه <strong>Transformers</strong> کلاس <code>Trainer</code> را برای کمک به <strong>fine-tune</strong> مدل‌های پیش‌آموزش‌دیده فراهم می‌کند. پس از انجام تمام مراحل پیش‌پردازش داده‌ها، تنها چند مرحله باقی مانده برای تعریف <code>Trainer</code>.\n",
        "  </p>\n",
        "  <p>\n",
        "    یکی از چالش‌ها سرعت کند اجرای <code>Trainer.train()</code> در <strong>CPU</strong> است که بهتر است از <strong>GPU</strong> استفاده شود.\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQjfDtl3jGJb"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    کد زیر در بخش قبلی اجرایی و توضیح داده شد:\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "RbVVsriQkwlv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoylQ2ADjGJc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    <strong>آموزش</strong>\n",
        "  </p>\n",
        "  <p>\n",
        "    برای شروع آموزش، ابتدا باید کلاس <code>TrainingArguments</code> را تعریف کنید که هایپرپارامترهای مورد نیاز برای آموزش و ارزیابی را شامل می‌شود. تنها آرگومانی که باید مشخص کنید، دایرکتوری ذخیره مدل و چک‌پوینت‌ها است، و برای باقی موارد می‌توانید از مقادیر پیش‌فرض استفاده کنید.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "Ze9_nbe7k3fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-ZhyCSJjGJd"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test-trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    مرحله دوم، تعریف مدل است. مانند فصل قبلی، از کلاس <strong>AutoModelForSequenceClassification</strong> استفاده خواهیم کرد، با دو برچسب:\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "kLQIz2uEk9lb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z6lTlZRjGJe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA2UHgssjGJe"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt3Rlo3JjGJf"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gicqzZZgjGJf",
        "outputId": "8cc2c88c-afd7-4825-a3e0-77edb21cc6f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(408, 2) (408,)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrj7zGnkjGJg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-6UiiVjjGJg",
        "outputId": "67429baa-1202-48aa-e835-c382b4f0bc32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQtSoPo9jGJh"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLWwa0_TjGJh"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1GAJPRSjGJh"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fine-tuning a model with the Trainer API or Keras",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}