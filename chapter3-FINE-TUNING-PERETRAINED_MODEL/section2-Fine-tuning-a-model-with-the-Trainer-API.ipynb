{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSmIyXijQGns"
      },
      "source": [
        "# Fine-tuning a model with the Trainer API"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    کتابخانه <strong>Transformers</strong> کلاس <code>Trainer</code> را برای کمک به <strong>fine-tune</strong> مدل‌های پیش‌آموزش‌دیده فراهم می‌کند. پس از انجام تمام مراحل پیش‌پردازش داده‌ها، تنها چند مرحله باقی مانده برای تعریف <code>Trainer</code>.\n",
        "  </p>\n",
        "  <p>\n",
        "    یکی از چالش‌ها سرعت کند اجرای <code>Trainer.train()</code> در <strong>CPU</strong> است که بهتر است از <strong>GPU</strong> استفاده شود.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "nfCn08NgbOgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f96VSVFYQGnv"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    کد زیر در بخش قبلی اجرایی و توضیح داده شد:\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "3G6PjQq8ecjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umyLNhrVQGnw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    <strong>آموزش</strong>\n",
        "  </p>\n",
        "  <p>\n",
        "    برای شروع آموزش، ابتدا باید کلاس <code>TrainingArguments</code> را تعریف کنید که هایپرپارامترهای مورد نیاز برای آموزش و ارزیابی را شامل می‌شود. تنها آرگومانی که باید مشخص کنید، دایرکتوری ذخیره مدل و چک‌پوینت‌ها است، و برای باقی موارد می‌توانید از مقادیر پیش‌فرض استفاده کنید.\n",
        "  </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "SUXn-e_Qf7zW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VHCLPEYEQGnx"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test-trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    مرحله دوم، تعریف مدل است. مانند فصل قبلی، از کلاس <strong>AutoModelForSequenceClassification</strong> استفاده خواهیم کرد، با دو برچسب:\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "9e4WOYTchMtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MQ2a-yNQGnx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVlcgFjZQGny"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjLb8EYlQGny"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E3fTRkOQGnz",
        "outputId": "b4447c97-d4e3-4e95-fbc0-23f7c40f11d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(408, 2) (408,)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6F5WvIxQGn0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-fVvx0-QGn0",
        "outputId": "fe069ffd-99ff-441c-90af-9e5fc0067a2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqloobi4QGn0"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHz9s9rDQGn1"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maE94WE2QGn1"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fine-tuning a model with the Trainer API or Keras",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}