{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mojtaba-Choopani/huggingface-llm-course-fa-notebooks/blob/main/chapter3-FINE-TUNING-PERETRAINED_MODEL/section1-Processing-the-data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdktaB-QKw3j"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 24px;\"> پردازش داده‌ها </b>\n",
        "\n",
        "</div>\n",
        "\n",
        "# Processing the data (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJd6wBY3Kw3z"
      },
      "source": [
        "<div dir=\"rtl\"> <p> نحوه آموزش یک Sequence Classifier روی یک batch </p> </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0UkdGEZSKw33",
        "outputId": "96f70bcd-c006-40d4-ddd1-9ca1d80fd7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.6.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zQ4H1XJKKw36",
        "outputId": "4f8c03b1-888a-4c78-eca0-0531558db902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-836499163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Same as before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Same as before\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"This course is amazing!\",\n",
        "]\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# This is new\n",
        "batch[\"labels\"] = torch.tensor([1, 1])\n",
        "\n",
        "optimizer = AdamW(model.parameters())\n",
        "loss = model(**batch).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    در این بخش از مجموعه داده <strong>MRPC</strong> استفاده می‌کنیم که شامل ۵۸۰۱ جفت جمله با برچسب هم‌معنی بودن یا نبودن است. این مجموعه کوچک است و برای آزمایش و آموزش مدل مناسب می‌باشد.\n",
        "  </p>\n",
        "</div>\n",
        "---"
      ],
      "metadata": {
        "id": "8WAAdzPgP22m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    هاب Hugging Face علاوه بر مدل‌ها، شامل مجموعه داده‌های متنوع در زبان‌های مختلف نیز هست. در این بخش، تمرکز بر روی مجموعه داده <strong>MRPC</strong> است که یکی از ۱۰ مجموعه داده معیار <strong>GLUE</strong> برای ارزیابی عملکرد مدل‌های یادگیری ماشین در ۱۰ وظیفه دسته‌بندی متن می‌باشد. با کتابخانه 🤗 <strong>Datasets</strong> می‌توان به سادگی این مجموعه داده را دانلود و ذخیره محلی کرد.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "_2BGK7nSSNBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3oCds4wKw38"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\"> <p> با اجرای دستور مربوطه، یک <strong>DatasetDict</strong> دریافت می‌کنیم که شامل سه بخش است: <em>training</em>، <em>validation</em> و <em>test</em>. هر بخش چند ستون دارد: <strong>sentence1</strong>، <strong>sentence2</strong>، <strong>label</strong> و <strong>idx</strong> و تعداد سطرها با تعداد جفت‌های جمله در هر بخش برابر است (مثلاً ۳,۶۶۸ جفت در <em>training</em>، ۴۰۸ در <em>validation</em> و ۱,۷۲۵ در <em>test</em>). </p> <p> این دستور مجموعه داده را دانلود و در حافظه محلی <code>~/.cache/huggingface/datasets</code> ذخیره می‌کند. می‌توان مسیر کش را با تنظیم متغیر محیطی <strong>HF_HOME</strong> تغییر داد. </p> <p> برای دسترسی به هر جفت جمله در <strong>raw_datasets</strong> می‌توان از اندیس‌گذاری مانند دیکشنری استفاده کرد. </p> </div>"
      ],
      "metadata": {
        "id": "r4QUMQ1gSud9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EwDpz5IGKw3-",
        "outputId": "508f2354-7113-4669-fdde-a5f8f647d3b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    می‌بینیم که برچسب‌ها (<strong>labels</strong>) قبلاً به صورت عددی هستند، بنابراین نیازی به پیش‌پردازش روی آن‌ها نداریم. برای فهمیدن این که هر عدد به کدام برچسب مربوط است، می‌توانیم ویژگی‌های <strong>raw_train_dataset</strong> را بررسی کنیم. این کار نوع هر ستون را به ما نشان می‌دهد.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "MJeeNYQjZoks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "    <p>\n",
        "    در پشت صحنه، ستون <strong>label</strong> از نوع <code>ClassLabel</code> است و نگاشت اعداد به نام برچسب‌ها در فولدر <strong>names</strong> ذخیره شده است.\n",
        "    عدد <strong>0</strong> به <code>not_equivalent</code> و عدد <strong>1</strong> به <code>equivalent</code> متناظر است.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "a2yZB23IaDDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BWuTBGWPKw4F",
        "outputId": "29ab84d0-44b1-4825-9623-87d0fb7352da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': Value('string'),\n",
              " 'sentence2': Value('string'),\n",
              " 'label': ClassLabel(names=['not_equivalent', 'equivalent']),\n",
              " 'idx': Value('int32')}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "raw_train_dataset.features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "    \n",
        "  <b style=\"font-size: 18px;\"> آماده‌سازی داده‌ها</b>\n",
        "  \n",
        "     برای آماده‌سازی داده‌ها برای مدل، متن‌ها باید با توکنایزر به اعداد تبدیل شوند. این کار شامل توکنیزه کردن همه جمله‌های اول و دوم هر جفت جمله است.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "-unRSHbscAfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qFYty_u1Kw4J"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"][:])\n",
        "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"][:])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    برای پیش‌بینی اینکه دو جمله هم معنی هستند یا نه، نمی‌توانیم فقط دو جمله را جداگانه به مدل بدهیم. باید جفت جملات را به صورت یک ورودی واحد پردازش کنیم و توکنایزر می‌تواند این کار را مطابق انتظار مدل BERT انجام دهد.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "dIM0Z2UPesFi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n4rGJUpRKw4L",
        "outputId": "40f38db9-2649-4d21-9d0e-489df4bb02bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    اگر مقادیر داخل <code>input_ids</code> را دوباره به کلمات تبدیل کنیم (decode کنیم):\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "g9Bo_TQKfTFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uc5Hfs7-Kw4R",
        "outputId": "ed6a6007-cd7c-405e-b66b-3d37ad2632da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " '.',\n",
              " '[SEP]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'one',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    <code>token_type_ids</code> مشخص می‌کند کدام بخش از ورودی مربوط به جمله اول و کدام مربوط به جمله دوم است.\n",
        "  </p>\n",
        "  <p>\n",
        "    همه مدل‌ها این مقادیر را ندارند؛ فقط مدل‌هایی که در پیش‌آموزش آن‌ها استفاده شده‌اند، این اطلاعات را نیاز دارند (مثل BERT).\n",
        "  </p>\n",
        "  <p>\n",
        "    BERT علاوه بر مدلسازی زبان ماسک‌شده، وظیفه پیش‌بینی جمله بعدی هم دارد تا رابطه بین جفت جمله‌ها را یاد بگیرد.\n",
        "  </p>\n",
        "  <p>\n",
        "    برای استفاده معمولی، کافی است از همان checkpoint برای توکنیزر و مدل استفاده کنید تا همه چیز درست کار کند.\n",
        "  </p>\n",
        "</div>\n",
        "---"
      ],
      "metadata": {
        "id": "v4yY_RrcfzCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    حالا که می‌دانیم توکنایزر چگونه یک جفت جمله را پردازش می‌کند، می‌توانیم کل مجموعه داده را توکنیزه کنیم.\n",
        "  </p>\n",
        "  <p>\n",
        "    برای این کار، لیست جملات اول و لیست جملات دوم را به توکنایزر می‌دهیم.\n",
        "  </p>\n",
        "  <p>\n",
        "    این روش با گزینه‌های <code>padding</code> و <code>truncation</code> که در فصل ۲ دیدیم، سازگار است.\n",
        "  </p>\n",
        "  <p>\n",
        "    به این ترتیب، داده‌ها آماده ورودی مدل می‌شوند و پیش‌پردازش مجموعه آموزش انجام می‌شود.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "_Ky7zIHYgUbm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "77Kk_yWvKw4V"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"][:],\n",
        "    raw_datasets[\"train\"][\"sentence2\"][:],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    توکنیزه کردن همه داده‌ها همزمان حافظه زیادی نیاز دارد و خروجی‌اش یک دیکشنری از لیست‌هاست، نه یک <code>Dataset</code> قابل استفاده مستقیم.\n",
        "  </p>\n",
        "  <p>\n",
        "    برای حفظ ساختار <code>Dataset</code> و استفاده بهینه از حافظه، از <code>Dataset.map()</code> استفاده می‌کنیم.\n",
        "  </p>\n",
        "  <p>\n",
        "    این متد روی هر نمونه از مجموعه داده تابعی اعمال می‌کند، پس می‌توانیم یک تابع تعریف کنیم که توکنیزیشن و پیش‌پردازش دلخواه را انجام دهد.\n",
        "  </p>\n",
        "  <p>\n",
        "    با استفاده از این روش، داده‌ها به شکل مناسبی برای مدل آماده می‌شوند و مصرف حافظه بهینه باقی می‌ماند.\n",
        "\n",
        "   یک نمونه کد پایتون برای استفاده از Dataset.map() همراه با توکنایزر:\n",
        "\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "RN0HWm5J1cml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ugsHDkcsKw4W"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    تابعی که تعریف می‌کنیم، یک دیکشنری از داده‌ها می‌گیرد و خروجی آن دیکشنری‌ای با کلیدهای <code>input_ids</code>، <code>attention_mask</code> و <code>token_type_ids</code> است.\n",
        "  </p>\n",
        "  <p>\n",
        "    این تابع می‌تواند روی چند نمونه به‌صورت هم‌زمان هم کار کند و به همین دلیل می‌توانیم در <code>Dataset.map()</code> از گزینه <code>batched=True</code> استفاده کنیم تا توکنیزه کردن سریع‌تر شود.\n",
        "  </p>\n",
        "  <p>\n",
        "    توکنیزر استفاده شده از کتابخانه 🤗 Tokenizers است که با زبان Rust نوشته شده و برای پردازش همزمان تعداد زیادی ورودی بسیار سریع است.\n",
        "  </p>\n",
        "  <p>\n",
        "    <strong>نکته:</strong> padding فعلاً در تابع اعمال نشده چون بهتر است هنگام ساختن batch انجام شود، نه برای کل مجموعه داده، تا مصرف حافظه و زمان کمتر شود.\n",
        "  </p>\n",
        "  <p>\n",
        "    در نهایت، با استفاده از <code>batched=True</code> می‌توانیم تابع را روی همه داده‌ها اعمال کنیم و پیش‌پردازش سریع‌تری داشته باشیم.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "c9vBpL9e2OjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tUBHvIW_Kw4Y",
        "outputId": "eac859e4-2e49-472f-ecbb-42565b81ea53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    کتابخانه 🤗 Datasets هنگام اعمال پیش‌پردازش، داده‌های اصلی را تغییر نمی‌دهد، بلکه ستون‌های جدیدی به مجموعه داده اضافه می‌کند.\n",
        "  </p>\n",
        "  <p>\n",
        "    هر کلید خروجی از تابع پیش‌پردازش (مثل <code>input_ids</code>، <code>attention_mask</code> و <code>token_type_ids</code>) به یک ستون جداگانه در Dataset تبدیل می‌شود.\n",
        "  </p>\n",
        "</div>\n",
        "---"
      ],
      "metadata": {
        "id": "cv7j8I4d4bir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    می‌توان از چندپردازشی (multiprocessing) هنگام استفاده از <code>map()</code> برای پیش‌پردازش استفاده کرد با پارامتر <code>num_proc</code>، اما اگر از توکنایزر سریع 🤗 استفاده می‌کنید، معمولاً لازم نیست چون خودش چند رشته‌ای کار می‌کند.\n",
        "  </p>\n",
        "  <p>\n",
        "    تابع <code>tokenize_function</code> یک دیکشنری با کلیدهای <code>input_ids</code>، <code>attention_mask</code> و <code>token_type_ids</code> برمی‌گرداند که به تمام بخش‌های مجموعه داده اضافه می‌شوند.\n",
        "  </p>\n",
        "  <p>\n",
        "    می‌توان مقادیر موجود در مجموعه داده را هم با همان کلیدها تغییر داد اگر تابع پیش‌پردازش مقدار جدید بدهد.\n",
        "  </p>\n",
        "  <p>\n",
        "    آخرین مرحله <strong>dynamic padding</strong> است: هنگام ایجاد batch، همه نمونه‌ها به طول طولانی‌ترین عنصر در آن batch پر می‌شوند تا مدل بتواند به صورت هم‌زمان پردازش کند.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "gh3Z-jNA4nfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "  <b style=\"font-size: 18px;\"> dynamic padding </b>\n",
        "  <p>\n",
        "    <strong>Collate function</strong>  نمونه‌ها را کنار هم در یک batch قرار می‌دهد. و برای ورودی‌هایی با طول متغیر، <strong>padding</strong> را به صورت دینامیک اعمال می‌کند تا آموزش سریع‌تر و بهینه‌تر انجام شود.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "VVFbRSb_6ZwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ybqqobbHKw4a"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    برای آزمایش این ابزار جدید، چند نمونه از مجموعه آموزش خود انتخاب می‌کنیم تا با هم در یک <strong>batch</strong> قرار دهیم. در اینجا ستون‌های <code>idx</code>، <code>sentence1</code> و <code>sentence2</code> را حذف می‌کنیم، زیرا نیازی به آن‌ها نداریم و حاوی رشته هستند (و نمی‌توان تنسور از رشته‌ها ساخت). سپس طول هر نمونه در <strong>batch</strong> را بررسی می‌کنیم.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "8Cq_brgU7bsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2N7uyZgIKw4c",
        "outputId": "cd04e162-cf0b-44b3-9ebc-feeb2642445f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50, 59, 47, 67, 59, 50, 62, 32]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    غیرمنتظره نیست که نمونه‌ها طول‌های متفاوتی دارند، از ۳۲ تا ۶۷. <strong>Dynamic padding</strong> یعنی نمونه‌های این <strong>batch</strong> باید همه تا طول ۶۷ (بیشترین طول در این batch) پر شوند. بدون dynamic padding، همه نمونه‌ها باید تا بیشترین طول کل مجموعه داده یا بیشترین طول قابل قبول مدل پر می‌شدند. حالا بیایید بررسی کنیم که <code>data_collator</code> ما به درستی <strong>batch</strong> را به‌صورت دینامیک پر می‌کند:\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "tnz0fAEN9LNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LoVAPixhKw4c",
        "outputId": "4352eb60-74c7-4028-ad70-a10dd0efaafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': torch.Size([8, 67]),\n",
              " 'token_type_ids': torch.Size([8, 67]),\n",
              " 'attention_mask': torch.Size([8, 67]),\n",
              " 'labels': torch.Size([8])}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "     حالا که از متن خام به <strong>batch</strong>هایی رسیدیم که مدل ما می‌تواند با آن‌ها کار کند، آماده‌ایم تا مدل را <strong>fine-tune</strong> کنیم!\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "wghGhCxe9U2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbkNJJda9kMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Processing the data (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}