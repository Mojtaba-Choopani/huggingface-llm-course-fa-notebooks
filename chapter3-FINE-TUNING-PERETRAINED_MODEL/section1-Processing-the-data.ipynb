{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mojtaba-Choopani/huggingface-llm-course-fa-notebooks/blob/main/chapter3-FINE-TUNING-PERETRAINED_MODEL/section1-Processing-the-data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdktaB-QKw3j"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 24px;\"> Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ </b>\n",
        "\n",
        "</div>\n",
        "\n",
        "# Processing the data (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJd6wBY3Kw3z"
      },
      "source": [
        "<div dir=\"rtl\"> <p> Ù†Ø­ÙˆÙ‡ Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Sequence Classifier Ø±ÙˆÛŒ ÛŒÚ© batch </p> </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0UkdGEZSKw33",
        "outputId": "96f70bcd-c006-40d4-ddd1-9ca1d80fd7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.6.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zQ4H1XJKKw36",
        "outputId": "4f8c03b1-888a-4c78-eca0-0531558db902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-836499163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Same as before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Same as before\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"This course is amazing!\",\n",
        "]\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# This is new\n",
        "batch[\"labels\"] = torch.tensor([1, 1])\n",
        "\n",
        "optimizer = AdamW(model.parameters())\n",
        "loss = model(**batch).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ <strong>MRPC</strong> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø´Ø§Ù…Ù„ ÛµÛ¸Û°Û± Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ù…â€ŒÙ…Ø¹Ù†ÛŒ Ø¨ÙˆØ¯Ù† ÛŒØ§ Ù†Ø¨ÙˆØ¯Ù† Ø§Ø³Øª. Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©ÙˆÚ†Ú© Ø§Ø³Øª Ùˆ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
        "  </p>\n",
        "</div>\n",
        "---"
      ],
      "metadata": {
        "id": "8WAAdzPgP22m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ù‡Ø§Ø¨ Hugging Face Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ØŒ Ø´Ø§Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ Ø¯Ø± Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù†ÛŒØ² Ù‡Ø³Øª. Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ <strong>MRPC</strong> Ø§Ø³Øª Ú©Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Û±Û° Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ÛŒØ§Ø± <strong>GLUE</strong> Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¯Ø± Û±Û° ÙˆØ¸ÛŒÙÙ‡ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙ† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø¨Ø§ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ğŸ¤— <strong>Datasets</strong> Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Ø³Ø§Ø¯Ú¯ÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­Ù„ÛŒ Ú©Ø±Ø¯.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "_2BGK7nSSNBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3oCds4wKw38"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\"> <p> Ø¨Ø§ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ± Ù…Ø±Ø¨ÙˆØ·Ù‡ØŒ ÛŒÚ© <strong>DatasetDict</strong> Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø´Ø§Ù…Ù„ Ø³Ù‡ Ø¨Ø®Ø´ Ø§Ø³Øª: <em>training</em>ØŒ <em>validation</em> Ùˆ <em>test</em>. Ù‡Ø± Ø¨Ø®Ø´ Ú†Ù†Ø¯ Ø³ØªÙˆÙ† Ø¯Ø§Ø±Ø¯: <strong>sentence1</strong>ØŒ <strong>sentence2</strong>ØŒ <strong>label</strong> Ùˆ <strong>idx</strong> Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø³Ø·Ø±Ù‡Ø§ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø¬ÙØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ø¯Ø± Ù‡Ø± Ø¨Ø®Ø´ Ø¨Ø±Ø§Ø¨Ø± Ø§Ø³Øª (Ù…Ø«Ù„Ø§Ù‹ Û³,Û¶Û¶Û¸ Ø¬ÙØª Ø¯Ø± <em>training</em>ØŒ Û´Û°Û¸ Ø¯Ø± <em>validation</em> Ùˆ Û±,Û·Û²Ûµ Ø¯Ø± <em>test</em>). </p> <p> Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ù…Ø­Ù„ÛŒ <code>~/.cache/huggingface/datasets</code> Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø³ÛŒØ± Ú©Ø´ Ø±Ø§ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ <strong>HF_HOME</strong> ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯. </p> <p> Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù‡Ø± Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ø¯Ø± <strong>raw_datasets</strong> Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² Ø§Ù†Ø¯ÛŒØ³â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯. </p> </div>"
      ],
      "metadata": {
        "id": "r4QUMQ1gSud9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EwDpz5IGKw3-",
        "outputId": "508f2354-7113-4669-fdde-a5f8f647d3b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒÙ… Ú©Ù‡ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ (<strong>labels</strong>) Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ø¯Ø¯ÛŒ Ù‡Ø³ØªÙ†Ø¯ØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ù†Ø¯Ø§Ø±ÛŒÙ…. Ø¨Ø±Ø§ÛŒ ÙÙ‡Ù…ÛŒØ¯Ù† Ø§ÛŒÙ† Ú©Ù‡ Ù‡Ø± Ø¹Ø¯Ø¯ Ø¨Ù‡ Ú©Ø¯Ø§Ù… Ø¨Ø±Ú†Ø³Ø¨ Ù…Ø±Ø¨ÙˆØ· Ø§Ø³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ <strong>raw_train_dataset</strong> Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ú©Ø§Ø± Ù†ÙˆØ¹ Ù‡Ø± Ø³ØªÙˆÙ† Ø±Ø§ Ø¨Ù‡ Ù…Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "MJeeNYQjZoks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "    <p>\n",
        "    Ø¯Ø± Ù¾Ø´Øª ØµØ­Ù†Ù‡ØŒ Ø³ØªÙˆÙ† <strong>label</strong> Ø§Ø² Ù†ÙˆØ¹ <code>ClassLabel</code> Ø§Ø³Øª Ùˆ Ù†Ú¯Ø§Ø´Øª Ø§Ø¹Ø¯Ø§Ø¯ Ø¨Ù‡ Ù†Ø§Ù… Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± <strong>names</strong> Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
        "    Ø¹Ø¯Ø¯ <strong>0</strong> Ø¨Ù‡ <code>not_equivalent</code> Ùˆ Ø¹Ø¯Ø¯ <strong>1</strong> Ø¨Ù‡ <code>equivalent</code> Ù…ØªÙ†Ø§Ø¸Ø± Ø§Ø³Øª.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "a2yZB23IaDDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BWuTBGWPKw4F",
        "outputId": "29ab84d0-44b1-4825-9623-87d0fb7352da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': Value('string'),\n",
              " 'sentence2': Value('string'),\n",
              " 'label': ClassLabel(names=['not_equivalent', 'equivalent']),\n",
              " 'idx': Value('int32')}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "raw_train_dataset.features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "    \n",
        "  <b style=\"font-size: 18px;\"> Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§</b>\n",
        "  \n",
        "     Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ ØªØ¨Ø¯ÛŒÙ„ Ø´ÙˆÙ†Ø¯. Ø§ÛŒÙ† Ú©Ø§Ø± Ø´Ø§Ù…Ù„ ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ Ùˆ Ø¯ÙˆÙ… Ù‡Ø± Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ø§Ø³Øª.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "-unRSHbscAfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qFYty_u1Kw4J"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"][:])\n",
        "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"][:])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ù‡Ù… Ù…Ø¹Ù†ÛŒ Ù‡Ø³ØªÙ†Ø¯ ÛŒØ§ Ù†Ù‡ØŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ÙÙ‚Ø· Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨Ø¯Ù‡ÛŒÙ…. Ø¨Ø§ÛŒØ¯ Ø¬ÙØª Ø¬Ù…Ù„Ø§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ÙˆØ±ÙˆØ¯ÛŒ ÙˆØ§Ø­Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†ÛŒÙ… Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø¯Ù„ BERT Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "dIM0Z2UPesFi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n4rGJUpRKw4L",
        "outputId": "40f38db9-2649-4d21-9d0e-489df4bb02bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ø§Ú¯Ø± Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø§Ø®Ù„ <code>input_ids</code> Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… (decode Ú©Ù†ÛŒÙ…):\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "g9Bo_TQKfTFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uc5Hfs7-Kw4R",
        "outputId": "ed6a6007-cd7c-405e-b66b-3d37ad2632da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " '.',\n",
              " '[SEP]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'one',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    <code>token_type_ids</code> Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ùˆ Ú©Ø¯Ø§Ù… Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¬Ù…Ù„Ù‡ Ø¯ÙˆÙ… Ø§Ø³Øª.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±Ø§ Ù†Ø¯Ø§Ø±Ù†Ø¯Ø› ÙÙ‚Ø· Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø¢Ù†â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù†Ø¯ (Ù…Ø«Ù„ BERT).\n",
        "  </p>\n",
        "  <p>\n",
        "    BERT Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ Ø²Ø¨Ø§Ù† Ù…Ø§Ø³Ú©â€ŒØ´Ø¯Ù‡ØŒ ÙˆØ¸ÛŒÙÙ‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¬Ù…Ù„Ù‡ Ø¨Ø¹Ø¯ÛŒ Ù‡Ù… Ø¯Ø§Ø±Ø¯ ØªØ§ Ø±Ø§Ø¨Ø·Ù‡ Ø¨ÛŒÙ† Ø¬ÙØª Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¹Ù…ÙˆÙ„ÛŒØŒ Ú©Ø§ÙÛŒ Ø§Ø³Øª Ø§Ø² Ù‡Ù…Ø§Ù† checkpoint Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¯Ø±Ø³Øª Ú©Ø§Ø± Ú©Ù†Ø¯.\n",
        "  </p>\n",
        "</div>\n",
        "---"
      ],
      "metadata": {
        "id": "v4yY_RrcfzCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ø­Ø§Ù„Ø§ Ú©Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒÙ… ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ú†Ú¯ÙˆÙ†Ù‡ ÛŒÚ© Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ù†ÛŒÙ….\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø±ØŒ Ù„ÛŒØ³Øª Ø¬Ù…Ù„Ø§Øª Ø§ÙˆÙ„ Ùˆ Ù„ÛŒØ³Øª Ø¬Ù…Ù„Ø§Øª Ø¯ÙˆÙ… Ø±Ø§ Ø¨Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø§ÛŒÙ† Ø±ÙˆØ´ Ø¨Ø§ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ <code>padding</code> Ùˆ <code>truncation</code> Ú©Ù‡ Ø¯Ø± ÙØµÙ„ Û² Ø¯ÛŒØ¯ÛŒÙ…ØŒ Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø¨Ù‡ Ø§ÛŒÙ† ØªØ±ØªÛŒØ¨ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¢Ù…Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "_Ky7zIHYgUbm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "77Kk_yWvKw4V"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"][:],\n",
        "    raw_datasets[\"train\"][\"sentence2\"][:],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù‡Ù…Ø²Ù…Ø§Ù† Ø­Ø§ÙØ¸Ù‡ Ø²ÛŒØ§Ø¯ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒØ§Ø´ ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø² Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§Ø³ØªØŒ Ù†Ù‡ ÛŒÚ© <code>Dataset</code> Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ….\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø± <code>Dataset</code> Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ØŒ Ø§Ø² <code>Dataset.map()</code> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø§ÛŒÙ† Ù…ØªØ¯ Ø±ÙˆÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªØ§Ø¨Ø¹ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù¾Ø³ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ÛŒÚ© ØªØ§Ø¨Ø¹ ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒÙ… Ú©Ù‡ ØªÙˆÚ©Ù†ÛŒØ²ÛŒØ´Ù† Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø´Ú©Ù„ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯.\n",
        "\n",
        "   ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Dataset.map() Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±:\n",
        "\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "RN0HWm5J1cml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ugsHDkcsKw4W"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ù† Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒØ§ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ <code>input_ids</code>ØŒ <code>attention_mask</code> Ùˆ <code>token_type_ids</code> Ø§Ø³Øª.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø±ÙˆÛŒ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù‡Ù…â€ŒØ²Ù…Ø§Ù† Ù‡Ù… Ú©Ø§Ø± Ú©Ù†Ø¯ Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¯Ø± <code>Dataset.map()</code> Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡ <code>batched=True</code> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… ØªØ§ ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù† Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø´ÙˆØ¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    ØªÙˆÚ©Ù†ÛŒØ²Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ğŸ¤— Tokenizers Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ Ø²Ø¨Ø§Ù† Rust Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ùˆ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ø²Ù…Ø§Ù† ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹ Ø§Ø³Øª.\n",
        "  </p>\n",
        "  <p>\n",
        "    <strong>Ù†Ú©ØªÙ‡:</strong> padding ÙØ¹Ù„Ø§Ù‹ Ø¯Ø± ØªØ§Ø¨Ø¹ Ø§Ø¹Ù…Ø§Ù„ Ù†Ø´Ø¯Ù‡ Ú†ÙˆÙ† Ø¨Ù‡ØªØ± Ø§Ø³Øª Ù‡Ù†Ú¯Ø§Ù… Ø³Ø§Ø®ØªÙ† batch Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŒ Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ ØªØ§ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø²Ù…Ø§Ù† Ú©Ù…ØªØ± Ø´ÙˆØ¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø¯Ø± Ù†Ù‡Ø§ÛŒØªØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² <code>batched=True</code> Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ØªØ§Ø¨Ø¹ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ù…Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†ÛŒÙ… Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ….\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "c9vBpL9e2OjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tUBHvIW_Kw4Y",
        "outputId": "eac859e4-2e49-472f-ecbb-42565b81ea53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ğŸ¤— Datasets Ù‡Ù†Ú¯Ø§Ù… Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø±Ø§ ØªØºÛŒÛŒØ± Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ØŒ Ø¨Ù„Ú©Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ù‡Ø± Ú©Ù„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø² ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ (Ù…Ø«Ù„ <code>input_ids</code>ØŒ <code>attention_mask</code> Ùˆ <code>token_type_ids</code>) Ø¨Ù‡ ÛŒÚ© Ø³ØªÙˆÙ† Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¯Ø± Dataset ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "  </p>\n",
        "</div>\n",
        "---"
      ],
      "metadata": {
        "id": "cv7j8I4d4bir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² Ú†Ù†Ø¯Ù¾Ø±Ø¯Ø§Ø²Ø´ÛŒ (multiprocessing) Ù‡Ù†Ú¯Ø§Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² <code>map()</code> Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± <code>num_proc</code>ØŒ Ø§Ù…Ø§ Ø§Ú¯Ø± Ø§Ø² ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø³Ø±ÛŒØ¹ ğŸ¤— Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù„Ø§Ø²Ù… Ù†ÛŒØ³Øª Ú†ÙˆÙ† Ø®ÙˆØ¯Ø´ Ú†Ù†Ø¯ Ø±Ø´ØªÙ‡â€ŒØ§ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    ØªØ§Ø¨Ø¹ <code>tokenize_function</code> ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ <code>input_ids</code>ØŒ <code>attention_mask</code> Ùˆ <code>token_type_ids</code> Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ú©Ù‡ Ø¨Ù‡ ØªÙ…Ø§Ù… Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ù‡Ù… Ø¨Ø§ Ù‡Ù…Ø§Ù† Ú©Ù„ÛŒØ¯Ù‡Ø§ ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯ Ø§Ú¯Ø± ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø¯Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ø¨Ø¯Ù‡Ø¯.\n",
        "  </p>\n",
        "  <p>\n",
        "    Ø¢Ø®Ø±ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ <strong>dynamic padding</strong> Ø§Ø³Øª: Ù‡Ù†Ú¯Ø§Ù… Ø§ÛŒØ¬Ø§Ø¯ batchØŒ Ù‡Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø·ÙˆÙ„ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ø¹Ù†ØµØ± Ø¯Ø± Ø¢Ù† batch Ù¾Ø± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ØªØ§ Ù…Ø¯Ù„ Ø¨ØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…â€ŒØ²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†Ø¯.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "gh3Z-jNA4nfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "  <b style=\"font-size: 18px;\"> dynamic padding </b>\n",
        "  <p>\n",
        "    <strong>Collate function</strong>  Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ù†Ø§Ø± Ù‡Ù… Ø¯Ø± ÛŒÚ© batch Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ùˆ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø·ÙˆÙ„ Ù…ØªØºÛŒØ±ØŒ <strong>padding</strong> Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¢Ù…ÙˆØ²Ø´ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØªØ± Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "VVFbRSb_6ZwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ybqqobbHKw4a"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø¬Ø¯ÛŒØ¯ØŒ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø®ÙˆØ¯ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¨Ø§ Ù‡Ù… Ø¯Ø± ÛŒÚ© <strong>batch</strong> Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ <code>idx</code>ØŒ <code>sentence1</code> Ùˆ <code>sentence2</code> Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ø²ÛŒØ±Ø§ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ Ù†Ø¯Ø§Ø±ÛŒÙ… Ùˆ Ø­Ø§ÙˆÛŒ Ø±Ø´ØªÙ‡ Ù‡Ø³ØªÙ†Ø¯ (Ùˆ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† ØªÙ†Ø³ÙˆØ± Ø§Ø² Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ Ø³Ø§Ø®Øª). Ø³Ù¾Ø³ Ø·ÙˆÙ„ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± <strong>batch</strong> Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "8Cq_brgU7bsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2N7uyZgIKw4c",
        "outputId": "cd04e162-cf0b-44b3-9ebc-feeb2642445f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50, 59, 47, 67, 59, 50, 62, 32]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "    ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ù†ÛŒØ³Øª Ú©Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø·ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø§Ø² Û³Û² ØªØ§ Û¶Û·. <strong>Dynamic padding</strong> ÛŒØ¹Ù†ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ† <strong>batch</strong> Ø¨Ø§ÛŒØ¯ Ù‡Ù…Ù‡ ØªØ§ Ø·ÙˆÙ„ Û¶Û· (Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø·ÙˆÙ„ Ø¯Ø± Ø§ÛŒÙ† batch) Ù¾Ø± Ø´ÙˆÙ†Ø¯. Ø¨Ø¯ÙˆÙ† dynamic paddingØŒ Ù‡Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ ØªØ§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø·ÙˆÙ„ Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ÛŒØ§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø·ÙˆÙ„ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ù…Ø¯Ù„ Ù¾Ø± Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯. Ø­Ø§Ù„Ø§ Ø¨ÛŒØ§ÛŒÛŒØ¯ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ… Ú©Ù‡ <code>data_collator</code> Ù…Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ <strong>batch</strong> Ø±Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ù¾Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯:\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "tnz0fAEN9LNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LoVAPixhKw4c",
        "outputId": "4352eb60-74c7-4028-ad70-a10dd0efaafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': torch.Size([8, 67]),\n",
              " 'token_type_ids': torch.Size([8, 67]),\n",
              " 'attention_mask': torch.Size([8, 67]),\n",
              " 'labels': torch.Size([8])}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p>\n",
        "     Ø­Ø§Ù„Ø§ Ú©Ù‡ Ø§Ø² Ù…ØªÙ† Ø®Ø§Ù… Ø¨Ù‡ <strong>batch</strong>Ù‡Ø§ÛŒÛŒ Ø±Ø³ÛŒØ¯ÛŒÙ… Ú©Ù‡ Ù…Ø¯Ù„ Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ø¢Ù†â€ŒÙ‡Ø§ Ú©Ø§Ø± Ú©Ù†Ø¯ØŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ§ÛŒÙ… ØªØ§ Ù…Ø¯Ù„ Ø±Ø§ <strong>fine-tune</strong> Ú©Ù†ÛŒÙ…!\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "wghGhCxe9U2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbkNJJda9kMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Processing the data (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}