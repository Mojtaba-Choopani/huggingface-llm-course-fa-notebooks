{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mojtaba-Choopani/huggingface-llm-course-fa-notebooks/blob/main/chapter2-USING-TRANSFORMERS/Behind-the-pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FcJdHNwayLv"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 30px;\">پشت پرده پایپ‌لاین در ترنسفورمرها</b>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FD8rdhaayLy"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<p>\n",
        "در فصل اول با تابع <code>pipeline()</code> آشنا شدیم.\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Ar4IgYayLz"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s808je6wayLz",
        "outputId": "011e3da7-407f-4e83-a9b3-8e6d843dbdde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<p>\n",
        "این پایپ‌لاین سه مرحله را در کنار هم قرار می‌دهد: پیش‌پردازش، عبور دادن ورودی‌ها از درون مدل، و پس‌پردازش.\n",
        "</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 18px;\"> پیش‌پردازش با توکنایزر</b>\n",
        "\n",
        "<p>\n",
        "مدل‌های ترنسفورمر نمی‌توانند متن خام را مستقیماً پردازش کنند، بنابراین ابتدا باید متن را به عدد تبدیل کنیم. این کار با <b>توکنایزر</b> انجام می‌شود که وظیفه‌اش:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>شکستن متن به توکن‌ها (کلمات، زیرکلمات یا نمادها)</li>\n",
        "  <li>تبدیل هر توکن به عدد</li>\n",
        "  <li>افزودن ورودی‌های اضافی لازم برای مدل</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "تمام این مراحل باید دقیقاً مطابق با آموزش اولیهٔ مدل انجام شود. برای همین، با استفاده از کلاس <code>AutoTokenizer</code> و متد <code>from_pretrained()</code>، توکنایزر مناسب مدل را از Model Hub بارگذاری می‌کنیم.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "برای مدل پیش‌فرض تحلیل احساسات (<code>distilbert-base-uncased-finetuned-sst-2-english</code>)، این کد اجرا می‌شود.\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "muoWc7gDgV68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk3UUrbIayL0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 18px;\"> تبدیل ورودی به تنسور</b>\n",
        "\n",
        "<p>\n",
        "پس از گرفتن توکنایزر، می‌توانیم جمله‌ها را به آن بدهیم و یک خروجی آماده برای مدل دریافت کنیم. تنها کار باقی‌مانده این است که شناسه‌های ورودی (<code>input IDs</code>) را به <b>تنسور</b> تبدیل کنیم.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        " <b>مدل‌های ترنسفورمر فقط تنسورها را به‌عنوان ورودی می‌پذیرند</b>، اما نگران فریم‌ورک (مثل PyTorch یا Flax) نباشید — کتابخانه  Transformers خودش آن را مدیریت می‌کند.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "اگر با تنسورها آشنایی ندارید، آن‌ها را مثل آرایه‌های NumPy تصور کنید:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>عدد (۰D)</li>\n",
        "  <li>بردار (۱D)</li>\n",
        "  <li>ماتریس (۲D) یا بیشتر</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "برای تعیین نوع تنسور، از آرگومان <code>return_tensors</code> استفاده می‌کنیم (مثلاً <code>\"pt\"</code> برای PyTorch).\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "c27lgfUuhdtp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ahgCagEayL1",
        "outputId": "8c523872-cc27-4b40-9de9-526b5e47e3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 18px;\"> خروجی توکنایزر چیست؟</b>\n",
        "\n",
        "<p>\n",
        "خروجی توکنایزر یک <b>دایکشنری</b> است با دو کلید اصلی:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li><code>input_ids</code>: شامل ردیف‌هایی از اعداد صحیح است که نمایانگر شناسه‌های توکن‌های هر جمله‌اند.</li>\n",
        "  <li><code>attention_mask</code>: در این فصل، بعداً توضیح داده می‌شود.</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "هر ردیف از <code>input_ids</code> مربوط به یک جملهٔ ورودی است.\n",
        "</p>\n",
        "---\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "_AMuV0URlrjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 18px;\"> عبور دادن ورودی از مدل</b>\n",
        "\n",
        "<p>\n",
        "می‌توانیم مدل پیش‌آموزش‌دیده را درست مانند توکنایزر بارگیری کنیم.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "کتابخانه  Transformers کلاسی به نام <code>AutoModel</code> را فراهم کرده که متدی به نام <code>from_pretrained()</code> دارد؛ دقیقاً مشابه توکنایزر.\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "akroo9MwmQod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S05wAveqayL1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<p>\n",
        "این معماری تنها شامل ماژول پایه‌ی ترنسفورمر است: با دریافت ورودی‌هایی، خروجی‌ای تولید می‌کند که آن را <b>بردارهای پنهان</b> می‌نامیم، که با نام <b>ویژگی‌ها</b> نیز شناخته می‌شوند — برداری با ابعاد بالا که نشان‌دهندهٔ درک مدل از معنای توکن در زمینهٔ جمله است.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "اگرچه این بردارهای پنهان به‌تنهایی می‌توانند مفید باشند، اما معمولاً به بخش دیگری از مدل داده می‌شوند که <b>«هد» (head)</b> نام دارد.\n",
        "در فصل اول دیدیم که وظایف مختلف می‌توانند با همین معماری پایه انجام شوند، اما هرکدام از این وظایف دارای <b>هد متفاوتی</b> هستند.\n",
        "</p>\n",
        "\n",
        "</div>\n",
        "---"
      ],
      "metadata": {
        "id": "Q97MuRCNr_bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 18px;\"> یک بردار با ابعاد بالا؟</b>\n",
        "\n",
        "<p>\n",
        "برداری که توسط ماژول ترنسفورمر تولید می‌شود معمولاً <b>بزرگ</b> است. این بردار معمولاً <b>سه بُعد</b> دارد:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li><b>Batch size:</b> تعداد دنباله‌هایی که هم‌زمان پردازش می‌شوند (در مثال ما ۲ مورد).</li>\n",
        "  <li><b>Sequence length:</b> طول نمایش عددی دنباله (در مثال ما ۱۶).</li>\n",
        "  <li><b>Hidden size:</b> بُعد بردار مربوط به هر ورودی مدل.</li>\n",
        "</ul>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "qx0vCpfGsr2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4USWDOsGayL1",
        "outputId": "a02eed96-bfd0-488e-d4e0-4f7c7bd4ebe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 768])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 18px;\"> هدهای مدل: معنا بخشیدن به اعداد</b>\n",
        "\n",
        "<p>\n",
        "هدهای مدل، <b>بردارهای پنهان با ابعاد بالا</b> را به‌عنوان ورودی می‌گیرند و آن‌ها را به بُعدی دیگر نگاشت می‌کنند. این هدها معمولاً از یک یا چند <b>لایه‌ی خطی (Linear Layer)</b> تشکیل شده‌اند.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "خروجی ماژول ترنسفورمر مستقیماً به هد مدل ارسال می‌شود تا پردازش شود.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "در کتابخانه  Transformers معماری‌های متنوعی وجود دارد که هرکدام برای یک وظیفه‌ی خاص طراحی شده‌اند. فهرست زیر کامل نیست، اما چند نمونه از آن‌ها را نشان می‌دهد:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li><code>Model</code> (برای دریافت بردارهای پنهان)</li>\n",
        "  <li><code>ForCausalLM</code></li>\n",
        "  <li><code>ForMaskedLM</code></li>\n",
        "  <li><code>ForMultipleChoice</code></li>\n",
        "  <li><code>ForQuestionAnswering</code></li>\n",
        "  <li><code>ForSequenceClassification</code></li>\n",
        "  <li><code>ForTokenClassification</code></li>\n",
        "  <li>و مدل‌های دیگر </li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "در مثال ما، به مدلی نیاز داریم که دارای <b>هد برای طبقه‌بندی دنباله</b> باشد (تا بتواند جملات را به مثبت یا منفی طبقه‌بندی کند). بنابراین، از کلاس <code>AutoModel</code> استفاده نمی‌کنیم، بلکه از <code>AutoModelForSequenceClassification</code> استفاده خواهیم کرد.\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "jIZFpcD2wtE0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AibBKHJQayL2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<p>\n",
        "اکنون اگر به <b>شکل (shape)</b> خروجی‌های خود نگاه کنیم، خواهیم دید که بُعد آن‌ها بسیار کمتر شده است: <b>هد مدل</b>، بردارهای پُربُعدی که قبلاً دیدیم را به‌عنوان ورودی می‌گیرد و <b>بردارهایی با دو مقدار</b> (یکی برای هر برچسب) به‌عنوان خروجی تولید می‌کند.\n",
        "از آن‌جایی که تنها دو جمله و دو برچسب داریم، خروجی‌ای که از مدل دریافت می‌کنیم دارای شکل ۲ × ۲ خواهد بود.\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "K6A_Qq1MyvpT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAhOguq2ayL2",
        "outputId": "dfac8860-d602-44d7-dd27-cdd5bf933813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<b style=\"font-size: 18px;\"> پردازش پس از خروجی (Postprocessing)</b>\n",
        "\n",
        "<p>\n",
        "مقدارهایی که به‌عنوان خروجی از مدل دریافت می‌کنیم، لزوماً به‌تنهایی معنا‌دار نیستند. بیایید نگاهی به آن‌ها بیندازیم:\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "Trjm8BzGzqMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC_bZSj2ayL2",
        "outputId": "ac635ba3-c436-468d-a773-1bf91d2a241e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.5607,  1.6123],\n",
            "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<p>\n",
        "مدل ما برای جملهٔ اول مقدارهای <code>[-1.5607, 1.6123]</code> و برای جملهٔ دوم مقدارهای <code>[4.1692, -3.3464]</code> را پیش‌بینی کرده است. این‌ها <b>احتمال نیستند</b>، بلکه <b>لاجیت</b> هستند — یعنی امتیازهای خام و نرمال‌نشده‌ای که از آخرین لایهٔ مدل خارج می‌شوند.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "برای تبدیل این مقادیر به احتمال، باید آن‌ها را از یک <b>لایه‌ی SoftMax</b> عبور دهیم.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "تمام مدل‌های  Transformers <b>لاجیت‌ها</b> را به‌عنوان خروجی برمی‌گردانند، زیرا در فرآیند آموزش، معمولاً <b>تابع فعال‌سازی نهایی</b> (مانند SoftMax) با <b>تابع زیان نهایی</b> (مانند cross-entropy) ترکیب می‌شود.\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "6anI_1ETzIkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qULfZMV0ayL2",
        "outputId": "f92b149f-9444-47c7-db1e-4718a5811690"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4.0195e-02, 9.5980e-01],\n",
              "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<p>\n",
        "اکنون می‌بینیم که مدل برای جملهٔ اول مقدارهای <code>[0.0402, 0.9598]</code> و برای جملهٔ دوم مقدارهای <code>[0.9995, 0.0005]</code> را پیش‌بینی کرده است. این‌ها <b>امتیازهایی هستند که به‌عنوان احتمال قابل‌تشخیص</b> شناخته می‌شوند.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "برای به‌دست آوردن <b>برچسب‌هایی که با هر موقعیت متناظر هستند</b>، می‌توانیم ویژگی <code>id2label</code> را در پیکربندی مدل بررسی کنیم (توضیحات بیشتر در بخش بعدی خواهد آمد).\n",
        "</p>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "saCQqyyg1Wrh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YziKJnUMayL3",
        "outputId": "d1629c08-a9b5-4182-f73f-d02b04afe892"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Behind the pipeline (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
