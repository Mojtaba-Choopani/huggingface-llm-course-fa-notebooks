### توکنایزر

توکنایزرها متن خام را به اعداد قابل فهم برای مدل تبدیل می‌کنند، چون مدل‌ها فقط با اعداد کار می‌کنند.  
هدف، یافتن نمایشی معنادار و ترجیحاً فشرده از متن است.  
الگوریتم‌های مختلفی برای این کار وجود دارد که در ادامه بررسی می‌شوند.

### توکنایزر مبتنی بر کلمه (Word-based)

اولین نوع توکنایزر که معمولاً به ذهن می‌رسد، توکنایزر مبتنی بر کلمه است.  
این روش تنظیمات ساده‌ای دارد و با چند قانون ساده کار می‌کند و در بسیاری از موارد نتایج مناسبی ارائه می‌دهد.  
در این روش، هدف این است که متن خام به کلمات تقسیم شود و برای هر کلمه یک نمایش عددی اختصاص یابد.

برای مثال:

```python
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
# خروجی: ['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

برخی نسخه‌های توکنایزر کلمه‌ای قوانین بیشتری برای علائم نگارشی دارند.  
در این نوع توکن‌سازی، معمولاً با واژگان (vocabulary) بزرگی روبه‌رو هستیم — واژگان یعنی مجموعه‌ی توکن‌های منحصربه‌فرد موجود در متن.

برای هر کلمه یک ID عددی اختصاص داده می‌شود (مثلاً از 0 تا اندازهٔ واژگان).  
مدل از این IDها برای شناسایی کلمات استفاده می‌کند.

**مشکلات توکنایزر مبتنی بر کلمه:**

- برای پوشش کامل یک زبان، باید برای هر کلمه یک شناسه داشته باشیم.  
  (مثلاً زبان انگلیسی بیش از ۵۰۰٬۰۰۰ کلمه دارد!)
- کلماتی مثل `dog` و `dogs` یا `run` و `running` شناسه‌های متفاوتی دارند  
  و مدل در ابتدا هیچ درکی از شباهت آن‌ها ندارد.
- برای کلماتی که در واژگان نیستند باید از توکن ناشناخته (`[UNK]` یا `<unk>`) استفاده شود.
- اگر تعداد زیادی توکن `[UNK]` تولید شود، نشانه‌ی از دست رفتن اطلاعات است.

### توکنایزر مبتنی بر کاراکتر (Character-based)

توکنایزرهای مبتنی بر کاراکتر، متن را نه بر اساس کلمات، بلکه به کاراکترها (حروف) تقسیم می‌کنند.  
این روش دو مزیت اصلی دارد:

- واژگان بسیار کوچکتری دارد.
- توکن‌های ناشناخته بسیار کمتری تولید می‌کند، چون هر کلمه از کاراکترها ساخته می‌شود و همهٔ کاراکترها در واژگان حضور دارند.

**چالش‌ها:**

اما این روش هم بی‌نقص نیست. مثلاً:

- **معنا کمتر می‌شود:** چون کاراکترها به‌تنهایی معنای خاصی ندارند (بر خلاف کلمات).
- در زبان‌هایی مانند چینی، هر کاراکتر اطلاعات بیشتری نسبت به زبان‌های لاتین دارد و این ایراد کمتر است.
- یک کلمه ممکن است به ۱۰ توکن یا بیشتر تبدیل شود که باعث افزایش حجم ورودی مدل می‌شود.

### توکن‌سازی زیرکلمه‌ای (Subword Tokenization)

الگوریتم‌های توکن‌سازی زیرکلمه‌ای بر این اصل تکیه دارند که:

- کلمات پرتکرار نباید به بخش‌های کوچک‌تر شکسته شوند
- ولی کلمات نادر باید به زیرکلمه‌های معنادار تجزیه شوند

مثلاً واژهٔ `annoyingly` که واژه‌ای کم‌کاربرد است، ممکن است به `annoying` و `ly` شکسته شود.  
این دو بخش به‌تنهایی کاربرد بیشتری دارند و در عین حال معنی اصلی کلمه حفظ می‌شود.

**مثال:**  
برای جملهٔ `Let’s do tokenization!`، الگوریتم توکن‌سازی زیرکلمه‌ای ممکن است چنین خروجی‌ای تولید کند:

```
Let, ’s, do, token, ization, !
```

اینجا کلمهٔ بلند `tokenization` به دو بخش `token` و `ization` شکسته شده که هر دو معنای مشخصی دارند،  
و در عین حال فقط با دو توکن نمایش داده شده‌اند، که از نظر حافظه و پردازش به‌صرفه است.


**مزایا:**

- پوشش زبانی خوب با واژگان کوچک
- تقریباً بدون تولید توکن‌های ناشناخته
- بسیار مناسب برای زبان‌های پیوندی (agglutinative) مثل ترکی استانبولی، که کلمات طولانی از ترکیب زیرکلمات ساخته می‌شوند.

### بارگذاری و ذخیره‌سازی توکنایزرها

بارگذاری و ذخیره‌سازی توکنایزرها همان‌قدر ساده است که برای مدل‌ها انجام می‌شود  
و دقیقاً از همان دو متد استفاده می‌شود: `from_pretrained()` و `save_pretrained()`.

این متدها:

- الگوریتم توکن‌سازی (مشابه معماری مدل)
- و واژگان آن (مشابه وزن‌های مدل)

را بارگذاری یا ذخیره می‌کنند.

---

**بارگذاری توکنایزر BERT:**

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```


مانند `AutoModel`، می‌توان از `AutoTokenizer` نیز استفاده کرد  
تا توکنایزر مناسب را بر اساس نام چک‌پوینت انتخاب کند:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```


**استفاده از توکنایزر:**

```python
tokenizer("Using a Transformer network is simple")
```

**خروجی:**

```python
{
  'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```


**ذخیره‌سازی توکنایزر:**

```python
tokenizer.save_pretrained("directory_on_my_computer")
```
