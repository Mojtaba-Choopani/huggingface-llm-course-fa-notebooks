# بهینه‌سازی استقرار مدل‌های زبان بزرگ (LLM)

در این بخش، فریم‌ورک‌های پیشرفته مانند **TGI**، **vLLM** و **llama.cpp** برای بهینه‌سازی استقرار مدل‌های زبان بزرگ (LLM) در محیط‌های تولید بررسی می‌شوند. تمرکز بر روی نحوه استقرار این فریم‌ورک‌ها در تولید و افزایش کارایی استنتاج است.

### راهنمای انتخاب فریم‌ورک

فریم‌ورک‌های **TGI**، **vLLM** و **llama.cpp** اهداف مشابهی دارند، اما ویژگی‌های خاصی دارند که آن‌ها را برای موارد استفاده مختلف مناسب‌تر می‌سازد. بیایید به تفاوت‌های کلیدی بین آن‌ها نگاه کنیم و تمرکز خود را بر عملکرد و یکپارچه‌سازی قرار دهیم.

### بهینه‌سازی استنتاج با TGI

فریم‌ورک **TGI** برای پایداری و پیش‌بینی‌پذیری در محیط تولید طراحی شده است و با استفاده از طول دنباله‌های ثابت و تکنیک‌هایی مانند **Flash Attention 2** و **batching مداوم** حافظه را به‌طور مؤثر مدیریت می‌کند. این امر باعث می‌شود که محاسبات توجه به‌طور کارآمد انجام شده و **GPU** به‌طور مداوم فعال باقی بماند. سیستم همچنین می‌تواند بخش‌هایی از مدل را بین **CPU** و **GPU** جابه‌جا کند تا مدل‌های بزرگ‌تر را مدیریت کند.

در **Flash Attention** مکانیزم توجه را در مدل‌های ترنسفورمر بهینه‌سازی می‌کند و گلوگاه‌های پهنای باند حافظه را رفع می‌کند. این تکنیک انتقال حافظه بین حافظه با پهنای باند بالا (**HBM**) و کش سریع‌تر **SRAM** را بهینه می‌کند و باعث کاهش زمان بی‌کاری **GPU** می‌شود. این روش استفاده از **VRAM** را کاهش داده و کارایی را بهبود می‌بخشد، که هم در آموزش و هم در استنتاج مفید است و موجب ارائه سریع‌تر و مقیاس‌پذیرتر LLM‌ها می‌شود.

### بهینه‌سازی استنتاج با vLLM

فریم‌ورک **vLLM** از تکنیک **PagedAttention** استفاده می‌کند که حافظه مدل را به بلوک‌های کوچکتر تقسیم می‌کند، مشابه به نحوه مدیریت حافظه در کامپیوترها با صفحات. این سیستم باعث می‌شود که مدل بتواند درخواست‌های با اندازه‌های مختلف را به‌صورت انعطاف‌پذیرتر پردازش کند و از هدر رفتن فضای حافظه جلوگیری کند.

در **PagedAttention** به مشکلات مدیریت حافظه کش **KV** در استنتاج **LLM** می‌پردازد. کش **KV** به صفحات با اندازه ثابت تقسیم می‌شود و به‌طور غیرمستقیم در حافظه **GPU** ذخیره می‌شود که امکان تخصیص حافظه انعطاف‌پذیرتر را فراهم می‌کند. یک جدول صفحات، صفحات مربوط به هر دنباله را پیگیری کرده و دسترسی به آن‌ها را بهینه می‌کند. حافظه می‌تواند بین درخواست‌های مختلف به‌طور مشترک استفاده شود.

این روش می‌تواند تا ۲۴ برابر عملکرد بالاتری نسبت به روش‌های سنتی داشته باشد و به طور قابل توجهی در استقرار مدل‌های زبان بزرگ (**LLM**) در محیط تولید موثر باشد.

### بهینه‌سازی استنتاج با llama.cpp

فریم‌ورک **llama.cpp** یک پیاده‌سازی بهینه‌شده C/C++ است که برای اجرای مدل‌های **LLaMA** روی سخت‌افزار مصرف‌کننده طراحی شده است. این فریم‌ورک بر کارایی **CPU** تمرکز دارد و از شتاب‌دهی **GPU** اختیاری پشتیبانی می‌کند. همچنین از تکنیک‌های **کوانتیزاسیون** برای کاهش اندازه مدل و نیازهای حافظه استفاده می‌کند در حالی که عملکرد خوبی را حفظ می‌کند. کوانتیزاسیون دقت وزن‌های مدل را از **۳۲-bit** یا **۱۶-bit** نقطه شناور به فرمت‌های دقت پایین‌تر مانند **۸-bit اعداد صحیح (INT8)**، **۴-bit** و حتی **۲-bit** کاهش می‌دهد. این کار باعث کاهش قابل توجه استفاده از حافظه و افزایش سرعت استنتاج با کمترین افت کیفیت می‌شود.


**ویژگی‌های کلیدی کوانتیزاسیون در llama.cpp:**

- **چندین سطح کوانتیزاسیون:** پشتیبانی از ۸-bit، ۴-bit، ۳-bit و حتی ۲-bit کوانتیزاسیون
- **فرمت GGML/GGUF:** استفاده از فرمت‌های تنسور سفارشی بهینه‌شده برای استنتاج کوانتیزه‌شده
- **دقت ترکیبی:** امکان اعمال سطوح مختلف کوانتیزاسیون به بخش‌های مختلف مدل
- **بهینه‌سازی‌های خاص سخت‌افزار:** شامل مسیرهای کد بهینه‌شده برای معماری‌های مختلف **CPU** (AVX2، AVX-512، **NEON**)

این رویکرد امکان اجرای مدل‌های با میلیاردها پارامتر را روی سخت‌افزار مصرف‌کننده با حافظه محدود فراهم می‌کند، که آن را برای استقرارهای محلی و دستگاه‌های مانند گوشی‌های هوشمند یا دوربین‌ها ایده‌آل می‌سازد.

### استقرار و یکپارچه‌سازی

پلتفرم **TGI** برای استقرار در سطح سازمانی طراحی شده و دارای ویژگی‌هایی مانند:

- پشتیبانی از **Kubernetes**
- پایش (Prometheus و Grafana)
- مقیاس‌گذاری خودکار
- ویژگی‌های امنیتی جامع

---

پلتفرم **vLLM** رویکردی انعطاف‌پذیرتر دارد، با Python ساخته شده و می‌تواند به راحتی **API OpenAI** را جایگزین کند.  
این فریم‌ورک برای عملکرد بالا و انعطاف‌پذیری مناسب است و به‌ویژه با **Ray** برای مدیریت خوشه‌ها عملکرد خوبی دارد.

---

پلتفرم **llama.cpp** بر سادگی و قابلیت حمل تمرکز دارد و می‌تواند روی سخت‌افزارهای مختلف از جمله لپ‌تاپ‌های مصرف‌کننده و دستگاه‌های موبایل اجرا شود.  
این سرور **API** سازگار با **OpenAI** ارائه می‌دهد و فوت‌پرینت منابع بسیار کوچکتری دارد.

