# چگونه ترنسفورمرهای  وظایف را حل می‌کنند؟

در این بخش، با بررسی دقیق‌تر نشان می‌دهیم که مدل‌های ترنسفورمر چگونه این وظایف را حل می‌کنند و در پشت‌صحنه چه می‌گذرد.

اگرچه راه‌کارهای مختلفی برای انجام هر وظیفه وجود دارد و بعضی مدل‌ها ممکن است تکنیک‌های خاصی را پیاده‌سازی کنند، اما اغلب مدل‌های ترنسفورمری از یکی از این سه ساختار پیروی می‌کنند: **انکودر، دیکودر، یا ترکیب انکودر–دیکودر**. این انعطاف‌پذیری معماری، عامل اصلی قدرت آن‌هاست.

## الگوی کلی حل وظایف

اغلب وظایف، الگوی مشابهی دارند:

- داده‌ی ورودی وارد مدل می‌شود  
- مدل آن را پردازش می‌کند  
- خروجی برای یک وظیفه خاص تفسیر می‌شود

تفاوت‌ها در چگونگی آماده‌سازی داده، انتخاب نوع معماری، و روش پردازش خروجی نهفته است.

---

## مدل‌های ترنسفورمر برای زبان

مدل‌های زبانی هسته‌ی اصلی NLP مدرن هستند. آن‌ها با یادگیری الگوهای آماری بین کلمات، می‌توانند زبان انسان را درک و تولید کنند.

مدل‌های ترنسفورمر به‌طور کلی به سه دسته تقسیم می‌شوند:

- **فقط انکودر (مانند BERT)**: برای درک دوطرفه متن و مناسب وظایف طبقه‌بندی و درک معنایی
- **فقط دیکودر (مانند GPT)**: مناسب تولید متن و ادامه جملات
- **انکودر–دیکودر (مانند BART، T5)**: برای وظایف دنباله‌به‌دنباله مانند ترجمه و خلاصه‌سازی

### دو روش اصلی آموزش:

- **Masked Language Modeling (MLM)**: در مدل‌هایی مثل BERT، برخی توکن‌ها ماسک می‌شوند و مدل باید آن‌ها را بر اساس متن اطراف پیش‌بینی کند (دوطرفه)
- **Causal Language Modeling (CLM)**: در مدل‌هایی مثل GPT، فقط از سمت چپ (توکن‌های قبل) برای پیش‌بینی توکن بعدی استفاده می‌شود (تک‌جهته)

---

## وظایف مختلف در NLP

###  تولید متن (Text Generation)

- **مدل:** GPT-2
- **ساختار:** فقط دیکودر
- **ویژگی‌ها:** استفاده از byte pair encoding، attention ماسک‌شده، پیش‌بینی توکن بعدی
- **کاربردها:** تکمیل جمله، نوشتن متن، پاسخ به سؤال

---

###  طبقه‌بندی متن (Text Classification)

- **مدل:** BERT
- **ساختار:** فقط انکودر
- **روش:** استفاده از توکن [CLS] و افزودن لایه طبقه‌بندی
- **اهداف پیش‌آموزش:**  
  - MLM (پیش‌بینی توکن‌های ماسک‌شده)  
  - NSP (پیش‌بینی جمله بعدی)
- **کاربردها:** تحلیل احساسات، تشخیص اسپم، دسته‌بندی موضوعی

---

###  برچسب‌گذاری توکن (Token Classification)

- **مدل:** BERT
- **کاربردها:** تشخیص موجودیت نام‌دار (NER)، برچسب‌گذاری نقش دستوری
- **روش:** افزودن head برچسب‌زن روی بردارهای نهایی توکن‌ها

---

###  پاسخ به پرسش (Question Answering)

- **مدل:** BERT
- **روش:** شناسایی مکان شروع و پایان پاسخ در متن
- **ساختار:** انکودر + head شناسایی بازه

---

###  خلاصه‌سازی (Summarization)

- **مدل:** BART
- **ساختار:** انکودر–دیکودر
- **روش پیش‌آموزش:** خراب‌کردن ورودی و بازسازی آن (text infilling)
- **ویژگی:** پیش‌بینی توکن‌های حذف‌شده بدون نیاز به شبکه پیش‌بینی نهایی مانند BERT

---

###  ترجمه (Translation)

- **مدل:** BART یا T5
- **ساختار:** انکودر–دیکودر
- **روش:** انکودر زبان مبدأ + دیکودر زبان مقصد
- **ویژگی:** نسخه mBART برای ترجمه چندزبانه آموزش دیده است

---

## فراتر از متن: گفتار و بینایی

###  گفتار و صدا (Speech & Audio)

- **مدل:** Whisper
- **ساختار:** انکودر–دیکودر
- **داده آموزشی:** ۶۸۰,۰۰۰ ساعت صدای برچسب‌خورده
- **کاربردها:** تشخیص گفتار، ترجمه شنیداری، تشخیص زبان
- **ویژگی:** عملکرد قوی در حالت zero-shot بدون نیاز به تنظیم خاص

### کد نمونه برای استفاده از Whisper:

```python
from transformers import pipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-base.en"
)
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
# {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}

