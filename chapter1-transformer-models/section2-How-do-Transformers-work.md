# مدل‌های ترنسفورمر چگونه کار می‌کنند؟

در این بخش به بررسی معماری مدل‌های ترنسفورمر، مفاهیم کلیدی مانند **لایه‌های توجه (attention)**، **ساختار Encoder–Decoder**، **یادگیری انتقالی (transfer learning)** و روند آموزش آن‌ها می‌پردازیم.  
این محتوا ترجمه و خلاصه‌ای دقیق از دوره رسمی Hugging Face است.

>  این بخش نسبت به بخش‌های قبلی فنی‌تر است. اگر همه چیز را از ابتدا متوجه نشدید نگران نباشید، در ادامه‌ی دوره به این مفاهیم باز خواهیم گشت.

---

##  تاریخچه مختصر ترنسفورمرها

معماری Transformer در ژوئن ۲۰۱۷ برای ترجمه معرفی شد. مدل‌های مهم در این حوزه به ترتیب عبارت‌اند از:

- **2018 – GPT**: اولین مدل پیش‌آموزش‌دیده‌ی خودرگرسیو
- **2018 – BERT**: برای درک بهتر ساختار جملات
- **2019 – GPT-2**: نسخه قوی‌تر و بزرگ‌تر GPT
- **2019 – T5**: برای وظایف مختلف با معماری Encoder–Decoder
- **2020 – GPT-3**: عملکرد عالی بدون نیاز به fine-tuning (zero-shot)
- **2022 – InstructGPT**
- **2023 – LLaMA**
- **2023 – Mistral**: سریع‌تر و بهتر از LLaMA-2 با ۷B پارامتر
- **2024 – Gemma 2**: سبک و قابل رقابت با مدل‌های بزرگ‌تر
- **2024 – SmolLM2**: مدل کوچک با عملکرد چشم‌گیر روی دستگاه‌های کم‌منبع

### دسته‌بندی اصلی مدل‌ها:
- **GPT-like**: خودرگرسیو (Auto-regressive)
- **BERT-like**: خودانکودر (Autoencoding)
- **T5-like**: انکودر–دیکودر (Sequence-to-sequence)

---

##  ترنسفورمرها مدل‌های زبانی هستند

تمام این مدل‌ها به عنوان **Language Model** آموزش دیده‌اند. یعنی با حجم زیادی از متن خام و به صورت **خودنظارتی (self-supervised)** آموزش دیده‌اند.

###  آموزش مدل زبانی شامل دو نوع است:

- **Causal Language Modeling**: پیش‌بینی واژه‌ی بعدی با توجه به واژه‌های قبلی  
- **Masked Language Modeling**: پیش‌بینی واژه‌ی حذف‌شده (ماسک‌شده) در جمله

###  سپس مدل وارد مرحله‌ی تنظیم مجدد (Fine-tuning) می‌شود:
- با داده‌های برچسب‌دار روی یک وظیفه خاص آموزش می‌بیند.
- باعث صرفه‌جویی در زمان، داده، منابع و هزینه می‌شود.
- دانش آموخته‌شده از پیش‌آموزش به مدل منتقل می‌شود (Transfer Learning).

---

##  مدل‌های ترنسفورمر بزرگ و پرهزینه‌اند

اکثر مدل‌های موفق از میلیاردها پارامتر تشکیل شده‌اند و آموزش آن‌ها نیاز به منابع زیاد محاسباتی دارد که منجر به:

- مصرف انرژی بالا
- هزینه مالی زیاد
- اثرات زیست‌محیطی قابل توجه

###  راه‌حل:
**استفاده مجدد از وزن‌های آموزش‌دیده و اشتراک‌گذاری مدل‌ها** باعث کاهش هزینه و ردپای کربنی می‌شود.

###  ابزارهای تخمین اثر زیست‌محیطی:
- [ML CO2 Impact](https://mlco2.github.io/impact)
- [CodeCarbon](https://codecarbon.io) (در Transformers )

---

##  معماری کلی مدل ترنسفورمر

مدل ترنسفورمر از دو بخش اصلی تشکیل شده است:

- **Encoder**: دریافت ورودی و ساخت نمایش برداری از آن (درک معنا)
- **Decoder**: تولید خروجی دنباله‌ای با توجه به Encoder و وضعیت قبلی

###  کاربردها:

| نوع معماری       | کاربردها |
|------------------|----------|
| فقط Encoder       | طبقه‌بندی متن، تشخیص موجودیت (NER) |
| فقط Decoder       | تولید متن، تکمیل جمله |
| Encoder–Decoder  | ترجمه، خلاصه‌سازی، پرسش‌پاسخ |

---

##  لایه‌های Attention چیستند؟

**مکانیزم توجه (Attention)** به مدل کمک می‌کند تشخیص دهد هنگام بررسی یک واژه، باید به کدام واژه‌های دیگر در جمله توجه بیشتری کند.

### مثال:
در جمله‌ی «You like this course»  
برای ترجمه‌ی درست واژه‌ی "like"، مدل باید به "You" توجه کند چون صرف فعل در زبان مقصد به فاعل وابسته است.

---

##  معماری اولیه ترنسفورمر

در کاربرد ترجمه:

- Encoder کل جمله‌ی مبدأ را بررسی می‌کند.
- Decoder فقط می‌تواند به واژه‌های قبلی خودش نگاه کند.
- Decoder همچنین از خروجی‌های Encoder برای پیش‌بینی واژه بعدی استفاده می‌کند.
- از **attention mask** استفاده می‌شود تا مدل به آینده یا padding دسترسی نداشته باشد.

---

##  تفاوت Architecture و Checkpoint

- **Architecture (معماری)**: تعریف ساختار لایه‌های مدل (مثلاً BERT)
- **Checkpoint (چک‌پوینت)**: وزن‌های آموزش‌دیده‌ی مدل (مثلاً bert-base-cased)
- **Model (مدل)**: اصطلاح کلی که به هر دو اطلاق می‌شود

---

## نتیجه‌گیری

مدل‌های ترنسفورمر با معماری ماژولار، یادگیری خودنظارتی، و قدرت تطبیق بالا، ستون فقرات بسیاری از کاربردهای NLP مدرن هستند.  
در ادامه‌ی دوره، هر یک از این اجزا را به‌صورت عمیق‌تر بررسی خواهیم کرد.

