# معماری‌های ترنسفورمر

در بخش‌های پیشین، معماری کلی ترنسفورمر را معرفی کردیم و نشان دادیم این مدل‌ها چگونه وظایف مختلف را حل می‌کنند. در این بخش، به سه نوع معماری اصلی در مدل‌های ترنسفورمر می‌پردازیم و بررسی می‌کنیم که چه زمانی باید از هرکدام استفاده شود.

اکثر مدل‌های ترنسفورمری یکی از این سه معماری را به‌کار می‌گیرند:

- فقط انکودر (encoder-only)
- فقط دیکودر (decoder-only)
- ترکیبی انکودر–دیکودر (sequence-to-sequence)

---

## ۱. مدل‌های فقط انکودر

مدل‌های انکودر تنها از بخش انکودر ترنسفورمر بهره می‌برند. در این معماری، لایه‌های توجه در هر مرحله به تمام واژگان جمله‌ی ورودی دسترسی دارند و به‌دلیل توجه دوطرفه به متن، "مدل‌های خودرمزگذار" (auto-encoding) نیز نامیده می‌شوند.

**پیش‌تمرین:**  
معمولاً شامل خراب‌کردن جمله (مثلاً با ماسک‌گذاری تصادفی واژگان) و آموزش مدل برای بازسازی آن است.

**کاربردها:**  
- طبقه‌بندی جمله  
- تشخیص موجودیت‌های نام‌دار (NER)  
- پاسخ‌گویی استخراجی  

**نمونه‌های معروف:**  
- BERT  
- DistilBERT  
- ModernBERT

---

## ۲. مدل‌های فقط دیکودر

در این معماری، تنها دیکودر ترنسفورمر استفاده می‌شود. در هر مرحله، توجه فقط به واژگان قبلی در جمله محدود می‌شود. این مدل‌ها "خودبازگشتی" (auto-regressive) نام دارند.

**پیش‌تمرین:**  
پیش‌بینی واژه‌ی بعدی در متن.

**کاربردها:**  
- تولید متن (داستان‌نویسی، گفت‌وگو، کدنویسی)

**نمونه‌ها:**  
- GPT  
- LLaMA  
- Gemma  
- DeepSeek V3

**مراحل آموزش در LLMهای مدرن:**

1. **پیش‌تمرین:** پیش‌بینی توکن بعدی از متن‌های گسترده  
2. **تنظیم با دستور:** آموزش برای پیروی از دستورات

**قابلیت‌ها:**  
- تولید متن  
- خلاصه‌سازی  
- ترجمه  
- پاسخ به سؤال  
- تولید کد  
- استدلال  
- یادگیری با مثال کم (few-shot)

---

## ۳. مدل‌های انکودر–دیکودر (Sequence-to-Sequence)

این مدل‌ها از هر دو بخش انکودر و دیکودر استفاده می‌کنند. انکودر به تمام ورودی دسترسی دارد ولی دیکودر فقط به توکن‌های پیشین در خروجی دسترسی دارد.

**پیش‌تمرین:**  
بازسازی جمله‌ای که ورودی آن به‌نحوی خراب شده. در T5، چند کلمه با یک ماسک جایگزین می‌شود و مدل باید متن اصلی را بازسازی کند.

**کاربردها:**  
- ترجمه  
- خلاصه‌سازی  
- پاسخ‌گویی تولیدی  
- اصلاح گرامری  

**نمونه‌ها:**  
- T5  
- BART  
- mBART  
- Marian

---

##  راهنمای انتخاب معماری مناسب

| وظیفه NLP              | معماری پیشنهادی    | مثال مدل             |
|------------------------|---------------------|-----------------------|
| طبقه‌بندی متن         | انکودر              | BERT, RoBERTa         |
| تولید متن             | دیکودر              | GPT, LLaMA            |
| ترجمه                 | انکودر–دیکودر       | T5, BART              |
| خلاصه‌سازی            | انکودر–دیکودر       | T5, BART              |
| NER                   | انکودر              | BERT                  |
| پاسخ استخراجی         | انکودر              | BERT                  |
| پاسخ تولیدی           | دیکودر یا ترکیبی    | GPT, T5               |
| گفت‌وگوی تعاملی       | دیکودر              | GPT, LLaMA            |

**سؤالات راهنما:**

- آیا نیاز به درک دوطرفه دارید؟
- آیا وظیفه شما شامل تولید متن است یا تحلیل متن؟
- آیا باید توالی ورودی به توالی خروجی تبدیل شود؟

---

#  تکامل LLMها و مکانیزم‌های توجه

مدل‌های جدید برای مقابله با مشکل پیچیدگی O(n²) در توجه، راهکارهایی مانند موارد زیر دارند:

##  LSH Attention در Reformer

**LSH** مخفف *Locality-Sensitive Hashing* است.

###  مشکل اصلی:
در attention سنتی، هر توکن با تمام توکن‌ها مقایسه می‌شود؛ بسیار پرهزینه برای متن‌های بلند.

###  راه‌حل LSH:

1. توکن‌ها به کمک تابع هش به گروه‌هایی از توکن‌های مشابه (باکت‌ها) دسته‌بندی می‌شوند.  
2. برای هر query، فقط به کلیدهای داخل همان باکت توجه می‌شود.  
3. چند دور هش (مثلاً `n_rounds`) استفاده می‌شود و میانگین آن‌ها گرفته می‌شود.

**مزیت:** کاهش شدید مصرف حافظه و زمان، بدون افت کیفیت محسوس.

---

##  Local Attention در Longformer

###  ایده اصلی:
در بسیاری از وظایف، زمینه‌ی محلی (مثلاً چند کلمه قبل و بعد) برای درک کافی است.

### 🔧 ساختار:
- هر توکن فقط به یک **پنجره‌ی محلی** توجه دارد.  
- برخی توکن‌ها با **global attention** به کل توالی دسترسی دارند و بالعکس.  

**مزایا:**  
- صرفه‌جویی در محاسبات  
- حفظ توانایی مدل برای درک وسیع از طریق لایه‌های پیاپی  

---

##  Axial Positional Encoding در Reformer

در ترنسفورمرهای سنتی، موقعیت توکن‌ها با یک ماتریس بزرگ \[L × D\] کدگذاری می‌شود که حافظه‌بر است. در Reformer این ماتریس به دو ماتریس کوچکتر با ابعاد محوری تقسیم می‌شود (مثلاً طول و عرض)، و با به‌هم‌چسباندن آن‌ها موقعیت نهایی به‌دست می‌آید.

---

این معماری‌ها و مکانیزم‌های بهینه به شما کمک می‌کنند تا برای هر نوع وظیفه در NLP، مدل مناسب را انتخاب و بهینه‌سازی کنید.

